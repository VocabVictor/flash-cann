# Flash-CANN

Flash-Attention implementation for Huawei Ascend NPU using CANN (Compute Architecture for Neural Networks).

å°† Flash-Attention ç®—æ³•ç§»æ¤åˆ°åä¸ºæ˜‡è…¾ NPU å¹³å°ï¼Œä½¿ç”¨ CANN å¼‚æ„è®¡ç®—æ¶æ„å®ç°ã€‚

## é¡¹ç›®ç®€ä»‹ (Project Overview)

Flash-CANN æ˜¯ [Flash-Attention](https://github.com/Dao-AILab/flash-attention) åœ¨åä¸ºæ˜‡è…¾ NPU ä¸Šçš„å®ç°ã€‚Flash-Attention æ˜¯ä¸€ç§å¿«é€Ÿä¸”å†…å­˜é«˜æ•ˆçš„ç²¾ç¡®æ³¨æ„åŠ›ç®—æ³•ï¼Œé€šè¿‡ IO æ„ŸçŸ¥ä¼˜åŒ–å’Œåˆ†å—è®¡ç®—æŠ€æœ¯ï¼Œå®ç°äº†ï¼š

- âš¡ **2-4å€é€Ÿåº¦æå‡**ï¼šç›¸æ¯”æ ‡å‡† Attention
- ğŸ’¾ **O(N) ç©ºé—´å¤æ‚åº¦**ï¼šä» O(NÂ²) é™ä½åˆ° O(N)
- ğŸš€ **å‡å°‘ HBM è®¿é—®**ï¼šå‡å°‘çº¦ 9å€çš„å†…å­˜è®¿é—®æ¬¡æ•°

## æ ¸å¿ƒæŠ€æœ¯ (Key Technologies)

### Flash-Attention ä¼˜åŒ–åŸç†

1. **Tiling (åˆ†å—è®¡ç®—)**ï¼šå°†å¤§çŸ©é˜µåˆ†å—å¤„ç†ï¼Œé¿å…å®ä¾‹åŒ–å®Œæ•´çš„ NÃ—N æ³¨æ„åŠ›çŸ©é˜µ
2. **Kernel Fusion (ç®—å­èåˆ)**ï¼šå°†å¤šä¸ªæ“ä½œèåˆåˆ°ä¸€ä¸ª kernelï¼Œå‡å°‘å†…å­˜è¯»å†™
3. **IO-Awareness (IO æ„ŸçŸ¥)**ï¼šä¼˜åŒ– HBM â†” SRAM ä¹‹é—´çš„æ•°æ®ä¼ è¾“
4. **åœ¨çº¿ Softmax**ï¼šä½¿ç”¨ç»Ÿè®¡é‡å¢é‡æ›´æ–° softmaxï¼Œæ— éœ€å­˜å‚¨ä¸­é—´ç»“æœ

### GPU vs NPU æ¶æ„å¯¹æ¯”

| ç‰¹æ€§ | NVIDIA GPU (CUDA) | Huawei Ascend NPU (CANN) |
|------|-------------------|--------------------------|
| è®¡ç®—å•å…ƒ | CUDA Cores + Tensor Cores | AI Core (Cube + Vector + Scalar) |
| çŸ©é˜µè®¡ç®— | å¤š Core åä½œ / Tensor Core | Cube Unit (16Ã—16 ç¡¬ä»¶çŸ©é˜µä¹˜) |
| å†…å­˜å±‚æ¬¡ | HBM â†” Shared Memory | DDR â†” L1 â†” Unified Buffer |
| ç¼–ç¨‹æ¨¡å‹ | CUDA (çº¿ç¨‹/å—/ç½‘æ ¼) | Ascend C (AI Core ç¼–ç¨‹) |
| è®¾è®¡ç†å¿µ | é€šç”¨å¹¶è¡Œè®¡ç®— (GPGPU) | AI ä¸“ç”¨åŠ é€Ÿ |

## é¡¹ç›®ç›®æ ‡ (Goals)

- [ ] å®ç° Flash-Attention å‰å‘ä¼ æ’­ (Forward Pass)
- [ ] å®ç° Flash-Attention åå‘ä¼ æ’­ (Backward Pass)
- [ ] æ”¯æŒ FP16/BF16 æ•°æ®ç±»å‹
- [ ] æ”¯æŒ Causal Masking
- [ ] æ€§èƒ½ä¼˜åŒ–ä¸åŸºå‡†æµ‹è¯•
- [ ] Python æ¥å£å°è£…
- [ ] ä¸ PyTorch é›†æˆ

## ç›®å½•ç»“æ„ (Project Structure)

```
flash-cann/
â”œâ”€â”€ README.md           # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ csrc/              # C++/Ascend C æºä»£ç 
â”‚   â”œâ”€â”€ kernels/       # CANN kernel å®ç°
â”‚   â””â”€â”€ operators/     # ç®—å­å°è£…
â”œâ”€â”€ python/            # Python æ¥å£
â”œâ”€â”€ tests/             # æµ‹è¯•ç”¨ä¾‹
â”œâ”€â”€ benchmarks/        # æ€§èƒ½åŸºå‡†æµ‹è¯•
â””â”€â”€ docs/              # æ–‡æ¡£
```

## ç¯å¢ƒè¦æ±‚ (Requirements)

- åä¸ºæ˜‡è…¾ NPU (Ascend 910/910B æ¨è)
- CANN å·¥å…·é“¾ >= 8.0
- Python >= 3.8
- (å¯é€‰) MindStudio IDE

## æ„å»ºä¸å®‰è£… (Build & Installation)

```bash
# å¾…å®ç°
# TBD
```

## ä½¿ç”¨ç¤ºä¾‹ (Usage)

```python
# å¾…å®ç°
# TBD
```

## æŠ€æœ¯æŒ‘æˆ˜ (Technical Challenges)

### CUDA â†’ CANN ç§»æ¤è¦ç‚¹

1. **å¹¶è¡Œæ¨¡å‹è½¬æ¢**
   - GPU: å¤§é‡çº¿ç¨‹ (10k+) å¤„ç†å°ä»»åŠ¡
   - NPU: å°‘é‡ AI Coreï¼Œæ¯ä¸ªå¤„ç†å¤§å—çŸ©é˜µè¿ç®—

2. **å†…å­˜ç®¡ç†**
   - GPU Shared Memory â†’ NPU Unified Buffer
   - ç†è§£æ˜‡è…¾çš„ä¸‰çº§å­˜å‚¨å±‚æ¬¡

3. **ç®—å­æ˜ å°„**
   - CUDA Tensor Core â†’ NPU Cube Unit (16Ã—16 çŸ©é˜µä¹˜)
   - CUDA å‘é‡è¿ç®— â†’ NPU Vector Unit
   - çº¿ç¨‹åŒæ­¥ â†’ AI Core è°ƒåº¦

4. **æ€§èƒ½ä¼˜åŒ–**
   - å……åˆ†åˆ©ç”¨ Cube Unit è¿›è¡Œåˆ†å—çŸ©é˜µä¹˜
   - ä¼˜åŒ– Unified Buffer ä½¿ç”¨
   - å‡å°‘ Global Memory è®¿é—®

## å‚è€ƒèµ„æ–™ (References)

- [Flash-Attention è®ºæ–‡](https://arxiv.org/abs/2205.14135)
- [Flash-Attention GitHub](https://github.com/Dao-AILab/flash-attention)
- [CANN å®˜æ–¹æ–‡æ¡£](https://www.hiascend.com/document)
- [æ˜‡è…¾ç¤¾åŒº](https://www.hiascend.com/zh/)

## å¼€å‘çŠ¶æ€ (Development Status)

ğŸš§ **é¡¹ç›®åˆæœŸ** - æ­£åœ¨è§„åˆ’æ¶æ„å’Œå®ç°æ ¸å¿ƒ kernel

## è´¡çŒ® (Contributing)

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºå»ºè®®ï¼

## è®¸å¯è¯ (License)

å¾…å®š (TBD)

## è‡´è°¢ (Acknowledgments)

æœ¬é¡¹ç›®åŸºäº [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention) çš„ç ”ç©¶æˆæœã€‚
